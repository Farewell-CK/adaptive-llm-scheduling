# AdaSplit: Adaptive Split-Scheduling for Mixed-Workload LLM Inference

**AdaSplit** is a dynamic resource partitioning framework designed to optimize Large Language Model (LLM) inference performance under mixed workloads (combining short conversational chats and long-context analysis tasks).

## ðŸŽ¯ Goal
To maximize system throughput and minimize latency by dynamically allocating GPU resources between "Short" and "Long" task partitions based on real-time demand.

## ðŸ— System Architecture

### Hardware Abstraction
*   **Total Resources**: 8 GPUs.
*   **Units**: Abstraced into **4 Homogeneous Units** (Unit 1-4).
*   **Configuration**: Each Unit operates as an independent **vLLM** instance with Tensor Parallelism (TP)=2 and FP8 quantization.
*   **Endpoints**: Ports 8001, 8002, 8003, 8004.

### Components
*   **Control Plane**: A centralized **Router** (FastAPI) acting as the gateway. It implements **Late Binding**, holding requests in internal queues until a specific Worker slot becomes available.
*   **Data Plane**: The vLLM Workers responsible for the actual inference execution.

## ðŸ§  Core Algorithms

AdaSplit employs a **Two-Level Scheduling Framework**:

### 1. Macro-Layer: Q-HAP (Queue-aware Hysteretic Adaptive Partitioning)
Dynamically adjusts the number of workers assigned to the "Long" vs. "Short" pools.
*   **Logic**: Monitors the backlog in the Router's internal queues.
*   **Scale Up**: If `queue_long > 10`, reassign a Short worker to the Long pool.
*   **Scale Down**: If `queue_long < 2`, return a Long worker to the Short pool.
*   **Guardrails**: Always maintain at least 1 worker for Short tasks ($N_{short} â‰¥ 1$).
*   **Hysteresis**: A cooldown period (e.g., 5s) prevents oscillation during rapid traffic changes.

### 2. Micro-Layer: RASP (Risk-Aware Stealing Policy)
Allows idle workers to process tasks from the overloaded partition under strict safety conditions.
*   **Logic**: Enables "Work Stealing" during "OFF" periods.
*   **Rule**: A Short worker can steal a Long task **IF AND ONLY IF**:
    1.  Its own `queue_short` is empty.
    2.  The worker has been idle for $> \tau_{cool}$ (e.g., 2s).

## ðŸš¦ Concurrency Control (Crucial)
To fully utilize vLLM's **Continuous Batching** capabilities, AdaSplit moves beyond simple boolean (`is_busy`) locking.
*   **Mechanism**: **Semaphore/Counter**.
*   **Parameter**: `WORKER_CONCURRENCY_LIMIT` (Default: **8**).
*   **Logic**: A worker is considered "available" only if `active_requests < 8`. Requests exceeding this limit are held in the Router queue, which is essential for Q-HAP to detect saturation signals.

## ðŸ“‚ Project Structure

*   **`start/`**: Launch scripts for the vLLM environment.
    *   `start_vllm_4_instances.sh`: Starts the 4 vLLM units.
*   **`tools/`**: 
    *   `router_dynamic.py`: **Main Logic**. Implements AdaSplit (Q-HAP + RASP) with semaphore-based concurrency.
    *   `router_static.py`: Baseline router with fixed partitioning.
    *   `benchmark_client.py`: Load generator and performance measurement.
    *   `monitor_vllm.py`: Metric scraper (Prometheus format).
    *   `workload_gen.py`: Generates synthetic mixed traces.

---

# AdaSplit: æ··åˆè´Ÿè½½ä¸‹çš„è‡ªé€‚åº” LLM æŽ¨ç†èµ„æºåˆ†åŒº

**AdaSplit** æ˜¯ä¸€ä¸ªåŠ¨æ€èµ„æºåˆ†åŒºæ¡†æž¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ··åˆè´Ÿè½½ï¼ˆå³çŸ­æ–‡æœ¬å¯¹è¯ä¸Žé•¿æ–‡æœ¬åˆ†æžæ··åˆï¼‰ä¸‹çš„å¤§æ¨¡åž‹æŽ¨ç†æ€§èƒ½ã€‚

## ðŸŽ¯ é¡¹ç›®ç›®æ ‡
é€šè¿‡æ ¹æ®å®žæ—¶è´Ÿè½½éœ€æ±‚ï¼Œåœ¨â€œé•¿æ–‡æœ¬ï¼ˆLongï¼‰â€å’Œâ€œçŸ­æ–‡æœ¬ï¼ˆShortï¼‰â€ä»»åŠ¡åˆ†åŒºä¹‹é—´åŠ¨æ€åˆ†é… GPU èµ„æºï¼Œä»Žè€Œæœ€å¤§åŒ–ç³»ç»Ÿåžåé‡å¹¶æœ€å°åŒ–å»¶è¿Ÿã€‚

## ðŸ— ç³»ç»Ÿæž¶æž„

### ç¡¬ä»¶æŠ½è±¡
*   **èµ„æºæ€»é‡**: 8 å¼  GPU å¡ã€‚
*   **å•å…ƒåˆ’åˆ†**: æŠ½è±¡ä¸º **4 ä¸ªåŒæž„å•å…ƒ (Unit 1-4)**ã€‚
*   **é…ç½®**: æ¯ä¸ªå•å…ƒæ˜¯ä¸€ä¸ªç‹¬ç«‹è¿è¡Œçš„ **vLLM** å®žä¾‹ï¼ˆTP=2, FP8 é‡åŒ–ï¼‰ã€‚
*   **ç›‘å¬ç«¯å£**: 8001, 8002, 8003, 8004ã€‚

### ç»„ä»¶
*   **æŽ§åˆ¶å¹³é¢ (Control Plane)**: åŸºäºŽ FastAPI çš„ **Router**ï¼Œä½äºŽå®¢æˆ·ç«¯å’Œ Worker ä¹‹é—´ã€‚é‡‡ç”¨ **å»¶è¿Ÿç»‘å®š (Late Binding)** æœºåˆ¶ï¼Œè¯·æ±‚å…ˆåœ¨ Router å†…éƒ¨æŽ’é˜Ÿï¼Œç›´åˆ° Worker æ˜Žç¡®æœ‰ç©ºä½ï¼ˆä¿¡å·é‡å…è®¸ï¼‰æ—¶æ‰åˆ†å‘ã€‚
*   **æ•°æ®å¹³é¢ (Data Plane)**: è´Ÿè´£å®žé™…æŽ¨ç†çš„ vLLM Workerã€‚

## ðŸ§  æ ¸å¿ƒç®—æ³•

æˆ‘ä»¬éœ€è¦å®žçŽ°ä¸€å¥— **åŒå±‚è°ƒåº¦æ¡†æž¶ (Two-Level Scheduling Framework)**ï¼š

### 1. å®è§‚å±‚ï¼šQ-HAP (åŸºäºŽé˜Ÿåˆ—çš„è¿Ÿæ»žè‡ªé€‚åº”åˆ†åŒº)
æ ¹æ® Router å†…éƒ¨é˜Ÿåˆ—çš„ç§¯åŽ‹æƒ…å†µï¼ŒåŠ¨æ€è°ƒæ•´ Worker åœ¨ "Long" å’Œ "Short" ç»„ä¹‹é—´çš„è§’è‰²åˆ†é…ã€‚
*   **æ‰©å®¹ (Scale Up)**: å¦‚æžœé•¿æ–‡æœ¬é˜Ÿåˆ—ç§¯åŽ‹ `queue_long > 10`ï¼Œå¢žåŠ  Long ç»„çš„ Workerã€‚
*   **ç¼©å®¹ (Scale Down)**: å¦‚æžœé•¿æ–‡æœ¬é˜Ÿåˆ—ç§¯åŽ‹ `queue_long < 2`ï¼Œå‡å°‘ Long ç»„çš„ Workerã€‚
*   **å®‰å…¨æŠ¤æ **: å¿…é¡»å§‹ç»ˆä¿ç•™è‡³å°‘ 1 ä¸ª Worker ç»™çŸ­æ–‡æœ¬ä»»åŠ¡ ($N_{short} â‰¥ 1$)ã€‚
*   **è¿Ÿæ»ž (Hysteresis)**: è®¾ç½®å†·å´æ—¶é—´ï¼ˆå¦‚ 5ç§’ï¼‰ï¼Œé˜²æ­¢è§’è‰²é¢‘ç¹åˆ‡æ¢å¯¼è‡´éœ‡è¡ã€‚

### 2. å¾®è§‚å±‚ï¼šRASP (é£Žé™©æ„ŸçŸ¥å·¥ä½œçªƒå–)
å…è®¸ç©ºé—²çš„ Worker åœ¨å®‰å…¨çš„æƒ…å†µä¸‹â€œçªƒå–â€å¦ä¸€ç»„çš„ä»»åŠ¡ã€‚
*   **è§„åˆ™**: ä¸€ä¸ª Short ç»„çš„ Worker ä»…å½“æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰æ¡ä»¶æ—¶ï¼Œæ‰å…è®¸çªƒå–ä¸€ä¸ª Long ä»»åŠ¡ï¼š
    1.  å®ƒè‡ªå·±çš„é˜Ÿåˆ— `queue_short` æ˜¯ç©ºçš„ã€‚
    2.  è¯¥ Worker å·²ç»æŒç»­ç©ºé—²äº†è¶…è¿‡ `RASP_STEAL_COOLDOWN` (ä¾‹å¦‚ 2ç§’)ï¼Œè¿™æ„å‘³ç€å½“å‰å¤„äºŽæµé‡ä½Žè°·æœŸ (OFF Period)ã€‚

## ðŸš¦ å¹¶å‘æŽ§åˆ¶ (å…³é”®ç»†èŠ‚)
ä¸ºäº†å……åˆ†åˆ©ç”¨ vLLM çš„ **Continuous Batching (è¿žç»­æ‰¹å¤„ç†)** æ€§èƒ½ï¼Œæˆ‘ä»¬æ‘’å¼ƒäº†ç®€å•çš„å¸ƒå°”å€¼é”å®šã€‚
*   **æœºåˆ¶**: **ä¿¡å·é‡/è®¡æ•°å™¨ (Semaphore/Counter)**ã€‚
*   **å‚æ•°**: `WORKER_CONCURRENCY_LIMIT` (é»˜è®¤: **8**)ã€‚
*   **é€»è¾‘**: åªæœ‰å½“ Worker çš„ `active_requests < 8` æ—¶ï¼Œæ‰è®¤ä¸ºè¯¥ Worker æ˜¯â€œå¯ç”¨â€çš„ã€‚
*   **åŽŸç†**: 8 æ˜¯ Llama-3-70B åœ¨æˆ‘ä»¬ç¡¬ä»¶ä¸Šåžåé‡é¥±å’Œçš„æ‹ç‚¹ã€‚è¶…è¿‡ 8 ä¸ªçš„è¯·æ±‚å¿…é¡»æ»žç•™åœ¨ Router é˜Ÿåˆ—ä¸­ï¼Œä»¥ä¾¿è§¦å‘ Q-HAP çš„æ‰©å®¹ä¿¡å·ã€‚

## ðŸ“‚ æ–‡ä»¶ç»“æž„è¯´æ˜Ž (`/workspace/tools/`)

*   **`router_dynamic.py`**: **[æ ¸å¿ƒä»£ç ]** ä¸»è¦çš„ Router é€»è¾‘ã€‚å®žçŽ°äº†åŸºäºŽä¿¡å·é‡çš„å¹¶å‘æŽ§åˆ¶ï¼Œä»¥åŠå®Œæ•´çš„ Q-HAP å’Œ RASP ç®—æ³•ã€‚
*   **`router_static.py`**: åŸºçº¿ Routerï¼ˆå›ºå®šåˆ†åŒºç­–ç•¥ï¼‰ã€‚
*   **`monitor_vllm.py`**: ç›‘æŽ§æŒ‡æ ‡æŠ“å–è„šæœ¬ã€‚
*   **`benchmark_client.py`**: è´Ÿè½½ç”Ÿæˆä¸ŽåŽ‹æµ‹å®¢æˆ·ç«¯ã€‚
*   **`micro_bench.py`**: ç”¨äºŽæµ‹å®šæœ€ä½³å¹¶å‘æ•°çš„å¾®åŸºå‡†æµ‹è¯•è„šæœ¬ã€‚
